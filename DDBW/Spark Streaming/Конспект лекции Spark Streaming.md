Конспект лекции 
Apache Spark Streaming – Обработка потоковых данных в реальном времени

## 1. Введение в потоковую обработку данных

### Что такое потоковая обработка?

Представьте себе данные, которые генерируются непрерывно и не имеют чёткого конца. Это и есть поток данных (data stream).

Примеры:
- Клики пользователей на веб-сайте.
- Показания с датчиков IoT (температура, давление).
- Финансовые транзакции.
- Серверные логи.

Потоковая обработка (Stream Processing) — это парадигма обработки таких бесконечных потоков данных "на лету", по мере их поступления, с минимальной задержкой, что дает возможность принимать решения в реальном времени (например, обнаружение мошенничества).

### Spark Streaming (DStreams) vs. Structured Streaming

Spark Streaming (старый API): Основан на концепции DStream (Discretized Stream). Он разбивал поток данных на серию маленьких RDD (микро-батчей) и обрабатывал их. Идея была в том, что если уже написан код для пакетной обработки, использовать его же для потоковой, запуская в цикле этот потоковый код для каждого микро-батча с данными, поступившими за последнюю  секунду или минуту. К сожалению, полного повторного использования кода не получилось, потоковый API был сложнее и отличался от пакетного API.

Spark Structured Streaming (текущий API): Революционный подход. Вместо микро-батчей RDD, он представляет поток данных как бесконечную, постоянно дописываемую таблицу (DataFrame). Это позволило унифицировать API для пакетной и потоковой обработки. Если вы умеете работать со Spark DataFrame, вы уже почти умеете работать со Structured Streaming.

## 2. Ключевая концепция: Поток как бесконечная таблица

Это самая важная идея, которую нужно понять.

![Structured Streaming Model](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)
*(Источник: официальная документация Apache Spark)*

1.  Входной поток (Input Stream): Данные поступают из источника (например, топика Kafka). Spark воспринимает каждую новую запись как новую строку, добавленную в "Входную таблицу" (Input Table).
2.  Запрос (Query): Вы пишете запрос к этой "бесконечной" таблице, используя стандартные операции DataFrame API: `select`, `filter`, `groupBy`, `join` и т.д.
3.  Таблица результатов (Result Table): Spark постоянно вычисляет результат вашего запроса. Эта таблица обновляется по мере поступления новых данных.
4.  Выходной поток (Output Stream): Изменения в таблице результатов преобразуются в выходной поток и записываются в приёмник (sink).

Так один и тот же код используется для обработки и пакетных, и потоковых данных. Все оптимизации Spark SQL (Catalyst Optimizer, Tungsten) автоматически применяются и к потоковым запросам.

## 3. Основные компоненты Structured Streaming

### Источники (Sources)

File Source. Читает файлы из директории по мере их появления. Поддерживает CSV, JSON, Parquet, ORC. 
Kafka Source. Промышленный стандарт для потоковых систем. Читает данные из топиков Apache Kafka. Обеспечивает высокую пропускную способность и отказоустойчивость.
Socket Source. Простой текстовый сокет. Для отладки и локальных тестов. Не использовать в продакшене!
Rate Source. Встроенный источник для тестирования, генерирует данные с заданной скоростью.

### Приёмники (Sinks)

Console Sink. Выводит результат в консоль. Полезно для отладки.
File Sink. Сохраняет результат в файлы (CSV, JSON, Parquet).
Kafka Sink. Публикует результат в топик Kafka.
Foreach/ForeachBatch Sink. Позволяет выполнить произвольный код для каждой записи или для каждого микро-батча. Используется для записи в системы, не имеющие нативного sink'а (например, в PostgreSQL, ClickHouse, Redis).

### Триггеры (Triggers)

Триггер определяет, как часто Spark должен проверять наличие новых данных и обновлять результат.

По умолчанию (микро-батчинг): `processingTime='0 seconds'`. Spark обрабатывает все доступные данные, как только завершил предыдущую обработку.
Фиксированный интервал: `processingTime='1 minute'`. Запускать обработку каждые N секунд/минут.
Один раз: `once=True`. Обработать один батч данных и завершить работу. Полезно для миграции с batch-задач на streaming.
Непрерывный (Continuous): `continuous='1 second'`. Экспериментальный режим с очень низкой задержкой (~1 мс), но с рядом ограничений.

### Режимы вывода (Output Modes)

`append` (добавление): В приёмник пишутся только новые строки, добавленные в таблицу результатов с момента последнего триггера. ингПодходит для запросов без агрегаций.
`complete` (полный): В приёмник пишется вся таблица результатов целиком при каждом триггере. Используется для запросов с агрегациями (`groupBy`).
`update` (обновление): В приёмник пишутся только те строки, которые были обновлены в таблице результатов. Более сложный режим, полезен, когда приемник умеет принимать такие обновления.

## 4.  PySpark

Устанавливаем PySpark:
pip install pyspark

### Пример приемник умеет принимать такие обновления директории
Допустим, система складывает JSON-файлы с данными о событиях в директорию `/tmp/iot_events`. Мы хотим в реальном времени читать эти данные и выводить только строки, для которых выполняется некоторое условие.

```python
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampTy
spark = SparkSession\
    .builder \
    .appName("FileStreamExample") \
    .master("local[2]") \
    .getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

# Определение схемы данных (важно для streaming!)
# Spark не может автоматически выводить схему для потока
schema = StructType([
    StructField("deviceId", StringType(), True),
    StructField("temperature", DoubleType(), True),
    StructField("danger_level", StringType(), True),
    StructField("timestamp", TimestampType(), True)
])

# Создание потокового DataFrame из файлового источника
# Spark будет "следить" за директорией /tmp/iot_events
streaming_df = spark \
    .readStream \
    .schema(schema) \
    .option("maxFilesPerTrigger", 1) \
    .json("/tmp/iot_events") 

# Выбираем только события с уровнем опасности "HIGH"
high_danger_events = streaming_df.filter("danger_level = 'HIGH'")

# Запуск потокового запроса с записью в приёмник
# --- Вариант А: Вывод в консоль для отладки ---
# query = high_danger_events \
#     .writeStream \
#     .outputMode("append") \
#     .format("console") \
#     .trigger(processingTime='10 seconds') \
#     .start()

# --- Вариант Б: Запись в Parquet-файлы ---
query = high_danger_events \
    .writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", "/tmp/parquet_output") \
    .option("checkpointLocation", "/tmp/checkpoint_parquet") \
    .trigger(processingTime='10 seconds') \
    .start()
print("Streaming query has started...")
# Ждем завершения потока (в данном случае, вечно)
query.awaitTermination()
```

Как это запустить:
1.  Создать директории: `mkdir /tmp/iot_events /tmp/parquet_output /tmp/checkpoint_parquet`
2.  Сохранить код в файл `stream_app.py` и запустите его: `python stream_app.py`.
3.  В другом терминале начните "генерировать" данные, создавая JSON-файлы в `/tmp/iot_events`:
    ```bash
    echo '{"deviceId": "dev-01", "temperature": 95.5, "danger_level": "HIGH", "timestamp": "2023-10-27T10:00:00Z"}' > /tmp/iot_events/event1.json
    sleep 5
    echo '{"deviceId": "dev-02", "temperature": 30.1, "danger_level": "LOW", "timestamp": "2023-10-27T10:00:05Z"}' > /tmp/iot_events/event2.json
    sleep 5
    echo '{"deviceId": "dev-03", "temperature": 102.0, "danger_level": "HIGH", "timestamp": "2023-10-27T10:00:10Z"}' > /tmp/iot_events/event3.json
    ```
4.  Наблюдать за консолью или за появлением файлов в `/tmp/parquet_output`.

### Пример 2: WordCount из сетевого сокета (Stateful)

Это классический пример, который демонстрирует агрегацию с состоянием.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split
spark = SparkSession \
    .builder \
    .appName("NetworkWordCount") \
    .master("local[2]") \
    .getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

# Создание потокового DataFrame из сокета
# Spark будет слушать порт 9999 на localhost
lines = spark \
    .readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# Разбиваем строки на слова
# .alias("word") - даем имя новой колонке
words = lines.select(
   explode(
       split(lines.value, " ")
   ).alias("word")
)

# Cчитаем количество вхождений каждого слова
word_counts = words.groupBy("word").count()

# Запуск запроса с выводом в консоль
# outputMode("complete"), так как у нас есть агрегация
query = word_counts \
    .writeStream \
    .outputMode("complete") \
    .format("console") \
    .trigger(processingTime='5 seconds') \
    .start()

print("Streaming query has started. Type words in netcat terminal.")
query.awaitTermination()
```

Как это запустить:
1.  В одном терминале запустит `netcat` в режиме прослушивания порта 9999:
    ```bash
    nc -lk 9999
    ```
2.  В другом терминале запустить ваш Python-скрипт.
3.  В терминале с `netcat` вводить слова, разделенные пробелами (например, `hello spark hello world`), завершая нажатием Enter.
4.  Каждые 5 секунд в терминале с Python-скриптом будет обновляться таблица с подсчитанными словами.

## 5. Работа с состоянием (Stateful Streaming)

### Оконные функции (Windowing)

Часто нужно агрегировать данные не за всё время, а за определенный временной интервал (окно).

Tumbling Window (скачущее окно): Окна фиксированного размера, которые не пересекаются.
    Пример: "Подсчитать количество кликов каждую минуту".
Sliding Window (скользящее окно): Окна фиксированного размера, которые пересекаются.
    Пример: "Каждые 10 секунд показывать среднюю температуру за последнюю минуту".

Пример кода с Tumbling Window:

```python
from pyspark.sql.functions import window

# Предположим, у нас есть streaming_df с колонками "timestamp" и "value"
windowed_counts = streaming_df.groupBy(
    # Группируем по 10-минутным окнам на основе колонки "timestamp"
    window("timestamp", "10 minutes")
).count()

# Для Sliding Window: window("timestamp", "10 minutes", "5 minutes")
# Длительность окна 10 минут, сдвиг (slide) 5 минут.
```

#### Уровень воды (Watermarks)

Проблема: В реальном мире данные могут приходить с опозданием. Если мы агрегируем данные по окнам, как долго нам ждать опоздавшие записи для окна, которое уже прошло (например, для `10:00-10:10`)? Если ждать вечно, то состояние (state) будет расти бесконечно.

Решение: Watermark.
Вы говорите Spark: "Я готов ждать опоздавшие данные не более N минут. Всё, что придет позже, игнорируй".

Watermark устанавливается на основе времени события (event time), т.е. временной метки внутри самих данных.
Он позволяет Spark безопасно очищать старые состояния и финализировать результаты для окон.

Пример кода с Watermark:

```python
from pyspark.sql.functions import window

# Мы готовы ждать данные, опоздавшие на 10 минут
# Агрегируем по 5-минутным окнам
windowed_counts = streaming_df \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        window("timestamp", "5 minutes")
    ).count()
```
Важно: Для использования Watermark в режиме `append`, Spark будет выдавать результат только после того, как окно "закроется" с учетом Wаtermark.

## 6. Архитектура и отказоустойчивость

### Контрольные точки (Checkpoints)

Это самый важный механизм для обеспечения отказоустойчивости.

Checkpoint Location — это директория в распределенной файловой системе (HDFS, S3), куда Spark сохраняет:
1.  Прогресс чтения (Offsets): До какой записи в Kafka или до какого файла мы дочитали.
2.  Состояние (State): Все промежуточные агрегации (например, таблицы для `groupBy`).

Если ваше приложение (или драйвер) падает, вы можете просто перезапустить его. Spark прочитает последнюю контрольную точку и продолжит работу ровно с того места, где остановился, восстановив всё состояние. Без этого данные будут потеряны или обработаны повторно.

```python
query = my_stream \
    .writeStream \
    ...
    # Обязательно для stateful-операций и для продакшена!
    .option("checkpointLocation", "/path/to/my/checkpoint/dir") \
    .start()
```

### Гарантии доставки: Exactly-Once Semantics

Structured Streaming (в связке с отказоустойчивыми источниками и приёмниками, такими как Kafka) может обеспечить семантику "ровно одна доставка" (exactly-once). Это означает, что каждая запись из источника будет обработана и отражена в результате ровно один раз, даже в случае сбоев.

Это достигается за счёт:
Идемпотентных приёмников (Idempotent Sinks): Способность приёмника безопасно обрабатывать дубликаты (например, при повторной записи после сбоя).
Транзакционной записи: Запись в приёмник и сохранение offset'а в checkpoint происходит атомарно.

## 7. Заключение и лучшие практики

### Место Structured Streaming в архитектуре данных:

- ETL/ELT в реальном времени: Очистка, обогащение и трансформация данных на лету перед загрузкой в хранилище (DWH) или озеро данных (Data Lake).
- Мониторинг и оповещения: Анализ логов или метрик для обнаружения аномалий в реальном времени.
- Интерактивная аналитика: Данные для дашбордов, которые обновляются в реальном времени.

### Лучшие практики:

1.  Всегда указывайте схему (`.schema(...)`): Не полагайтесь на автоматическое определение схемы в потоках.
2.  Всегда используйте `checkpointLocation`: Это критично для отказоустойчивости, особенно для stateful-операций.
3.  Используйте Watermark (`withWatermark`) при работе с окнами и временем события, чтобы избежать бесконечного роста состояния.
4.  Мониторьте свои стриминговые запросы: Используйте Spark UI для отслеживания `inputRate`, `processingRate` и `latency`. Если `processingRate` стабильно ниже `inputRate`, ваш кластер не справляется.
5.  Выбирайте правильный `outputMode`: `append` для простых потоков, `complete` или `update` для агрегаций.
6.  Для записи в кастомные системы (СУБД): Используйте `foreachBatch`. Это позволяет применять batch-запись в каждом микро-батче, что гораздо эффективнее, чем `foreach` (построчная запись).